{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Abrimos la conexión con la BBDD\n",
    "conn = sqlite3.connect(\"D:/master/Master-in-Data-Science/Proyecto/Datos/worldTimeSeries.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Vamos a intentar automatizar el análisis anterior, es decir, considerar los datos a partir de cierto año y a \n",
    "# continuacion filtrar por aquellas estaciones que tienen observaciones por año inferior a 364 dias\n",
    "df_total = DataFrame()\n",
    "#continentes=[\"europa\"]\n",
    "continentes=[\"antarctica\",\"central_america\",\"south_america\",\"europa\",\"africa\",\"asia\",\"oceania\",\"north_america\"]\n",
    "\n",
    "for cont in continentes:\n",
    "    print(\"Inicio continente: \" + cont + \" a las \" + str(time.strftime(\"%H:%M:%S\")))\n",
    "    \n",
    "    # Recuperamos todos los paises que aparecen en el continente\n",
    "    countries=[]\n",
    "    rows = cursor.execute(\"select distinct country from world_time_\"+cont+\";\")\n",
    "    for r in rows:\n",
    "        countries.append(r[0])\n",
    "\n",
    "    for country in countries:\n",
    "\n",
    "        \n",
    "        \n",
    "    # Se recuperan todas las estaciones del continentes que existían previamente a 1900 y existen en 2016\n",
    "    rows = cursor.execute(\"select distinct station from world_time_\"+cont+\" where date = '19000101';\")\n",
    "        \n",
    "    station_list_1900 = []\n",
    "    for r in rows:\n",
    "        station_list_1900.append({\"station\":r[0]})\n",
    "        \n",
    "    if len(station_list_1900) > 0:\n",
    "        rows = cursor.execute(\"select distinct station from world_time_\"+cont+\" where date = '20160401';\")        \n",
    "        station_list_2016 = []\n",
    "        for r in rows:\n",
    "            station_list_2016.append({\"station\":r[0]})\n",
    "\n",
    "        station_list_1900 = DataFrame(station_list_1900)\n",
    "        station_list_2016 = DataFrame(station_list_2016)\n",
    "\n",
    "        # Nos quedamos con las estaciones que estan en ambas listas\n",
    "\n",
    "        station_list = station_list_1900.merge(station_list_2016)\n",
    "        station_list = station_list.reset_index()\n",
    "\n",
    "        if len(station_list) > 0:\n",
    "            # Se compone la lista de estaciones para que lo entienda SQL, se seleccionan bloques de 500 estaciones\n",
    "            stations = []\n",
    "            stats = \"\"\n",
    "            n = 0;\n",
    "            for s in station_list.as_matrix():\n",
    "                n = n + 1\n",
    "                if stats != \"\":\n",
    "                    stats = stats + \",\"\n",
    "                stats = stats + \"'\" + s[1] + \"'\"\n",
    "                \n",
    "                if n % 100 == 0:\n",
    "                    stations.append(stats)\n",
    "                    stats = \"\"\n",
    "                    \n",
    "            if stats != \"\":\n",
    "                stations.append(stats)\n",
    "\n",
    "            # Se itera para cada uno de los bloques de 100 estaciones\n",
    "            n_lote = 0\n",
    "            while n_lote < len(stations):\n",
    "                print (\"   Iteracion sobre el bloque \" + str(n_lote) + \" de 100 estaciones de \" + str(len(stations)))\n",
    "                rows = cursor.execute(\"\"\"select \n",
    "                                        station,\n",
    "                                        date,\n",
    "                                        sum(TMAX) as TMAX, \n",
    "                                        sum(TMIN) as TMIN, \n",
    "                                        sum(PRCP) as PRCP,\n",
    "                                        count(*) as NOBS\n",
    "                                    from world_time_\"\"\"+cont+\"\"\" \n",
    "                                    where station in (\"\"\"+stations[n_lote]+\"\"\") \n",
    "                                    group by station, date\n",
    "                                    order by station, date;\"\"\")                \n",
    "\n",
    "                result=[]           \n",
    "                for r in rows:\n",
    "                    if int(r[1][:4]) >= 1900:\n",
    "                        result.append({\"station\":r[0],\"date\":r[1],\"TMAX\":r[2],\"TMIN\":r[3],\"PRCP\":r[4],\"NOBS\":r[5]})\n",
    "\n",
    "                df = DataFrame(result)\n",
    "                \n",
    "                result = []\n",
    "                n_lote = n_lote+1\n",
    "                \n",
    "                df.PRCP = df.PRCP.fillna(0)\n",
    "                df = df.dropna()\n",
    "\n",
    "                # Para reducir el tiempo que consume la query anterior, se hace la media anual de temperaturas \n",
    "                # fuera del SQL\n",
    "                # Se calcula el año a partir de la fecha\n",
    "                df['year'] = df.date.map(lambda x: int(x[:4]))\n",
    "\n",
    "                # Analizamos el numero de observaciones por estacion\n",
    "                df['n'] = 1\n",
    "                df_agg = df.groupby([\"station\",\"year\"]).sum()\n",
    "                df_agg = df_agg.reset_index()\n",
    "                df = DataFrame()\n",
    "\n",
    "                # Identificamos todas aquellas estaciones que tienen años con un numero de observaciones inferior \n",
    "                # a 364, excluyendo para ello el año 2016 (que no esta completo)\n",
    "                df_falta_datos = df_agg[df_agg.n < 364]\n",
    "                df_falta_datos = df_falta_datos[df_falta_datos.year != 2016]\n",
    "                df_falta_datos = df_falta_datos.drop_duplicates(\"station\")\n",
    "\n",
    "                # Se eliminan las estaciones identificadas\n",
    "                for s in df_falta_datos.station:\n",
    "                    df_agg = df_agg[df_agg.station != s]\n",
    "\n",
    "                # Se eliminan las variables no necesarias\n",
    "                df_agg = df_agg[['station','year','TMAX','TMIN','PRCP','NOBS']]\n",
    "\n",
    "                # Es necesario eliminar las series que tienen un año completo sin datos\n",
    "\n",
    "                # Agregamos los indicadores por estacion\n",
    "                df_agg_2 = df_agg.groupby('station').count()\n",
    "\n",
    "                # Identificamos aquellas estaciones que no llegan al número de años que deberían tener\n",
    "                if len(df_agg_2) > 0:\n",
    "                    df_agg_2 = df_agg_2[df_agg_2.year < max(df_agg_2.year)]\n",
    "\n",
    "                df_agg_2 = df_agg_2.reset_index()[['station']]\n",
    "\n",
    "                # Eliminamos las estaciones identificadas\n",
    "                for s in df_agg_2.station:\n",
    "                    df_agg = df_agg[df_agg.station != s]\n",
    "\n",
    "                df_agg_2 = DataFrame()\n",
    "                # Agregamos los indicadores por año\n",
    "                df_agg = df_agg.groupby('year').sum()\n",
    "                df_agg = df_agg.reset_index()\n",
    "                \n",
    "                df_agg['continent'] = cont\n",
    "                \n",
    "                df_total = df_total.append(df_agg)\n",
    "                df_agg = DataFrame()\n",
    "                df_total.to_csv(\"Resultado_calentamiento.csv\")\n",
    "    \n",
    "    print(\"Fin continente: \" + cont)    \n",
    "    \n",
    "    df_total.to_csv(\"Resultado_calentamiento.csv\")\n",
    "conn.close()\n",
    "\n",
    "# Una vez iterados por todos los continentes, se realiza una agrupacion por continente y año para obtener los \n",
    "# indicadores esperados\n",
    "\n",
    "df_total = df_total.groupby(['continent','year']).sum()\n",
    "df_total = df_total.reset_index()\n",
    "\n",
    "# Nos quedamos con las observaciones posteriores a 1900\n",
    "df_total = df_total[df_total.year >= 1900]\n",
    "\n",
    "# Se calculan los indicadores\n",
    "df_total.TMAX = df_total.TMAX/df_total.NOBS\n",
    "df_total.TMIN = df_total.TMIN/df_total.NOBS\n",
    "df_total.PRCP = df_total.PRCP/df_total.NOBS\n",
    "\n",
    "# Se almacena el resultado final\n",
    "df_total.to_csv(\"Resultado_calentamiento.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
